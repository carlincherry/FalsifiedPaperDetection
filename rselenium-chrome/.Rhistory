# feature to stabilize longer crawl jobs.
#
# arguments:
# - none
if (!exists("remDr", envir = globalenv())) {
stop('No active session')
}
old_browser <- remDr$browserName
remDr$closeall()
endSession()
Sys.sleep(1)
if (old_browser == "chrome") {
startSession(browser = "firefox")
} else if (old_browser == "firefox") {
startSession(browser = "chrome")
} else {
startSession(browser == sample(c("chrome", "firefox"), 1))
}
}
# microsoft-academic-functions.R ------------------------------------------
# Part of our web crawler. Adds functions to the environment for scraping
# Microsoft Academic.
#
# Needs Docker Desktop installed. No input.
#
# Outputs nothing.
#
# If there are questions/comments, please contact Matt (mdlee@usc.edu).
# -------------------------------------------------------------------------
# Loading required packages
suppressPackageStartupMessages(library("tidyverse"))
suppressPackageStartupMessages(library("lubridate"))
suppressPackageStartupMessages(library("RSelenium"))
suppressPackageStartupMessages(library("getProxy"))
microsoftSearchAuthor <- function(author, paper = "") {
message(paste0('Starting search for ', author, ' + ', substring(paper, 0, 30), '...'))
# Initialize author information
author_info <- data.frame(
first_author = author,
affiliation = as.character(NA),
publications = as.character(NA),
citations = as.numeric(NA),
year_start = as.numeric(NA),
year_end = as.numeric(NA),
top_topics = as.character(NA),
publication_types = as.character(NA),
top_authors = as.character(NA),
top_journals = as.character(NA),
top_institutions = as.character(NA),
top_conferences = as.character(NA),
stringsAsFactors = FALSE
)
# If not already on the Microsoft Academic page, navigate there and wait for header image
if (unlist(remDr$getCurrentUrl()) != "https://academic.microsoft.com/home") {
remDr$navigate("https://academic.microsoft.com/home")
}
webElem <- findElementCustom(using = "css selector", value = ".top", attempts = 20)
remDr$screenshot(display = TRUE)
# Find search box and enter author + paper
webElem <- findElementCustom(using = "xpath", value = "/html/body/div/div/div/router-view/div/div[1]/div[2]/div/ma-suggestion-control/div/div[1]/input")
webElem$clearElement()
webElem$clickElement()
webElem$sendKeysToElement(list(paste(author, paper)))
webElem$sendKeysToElement(list(key = "tab"))
remDr$screenshot(display = TRUE)
# Submit search and wait for author card to load
webElem <- findElementCustom(using = "xpath", value = "/html/body/div/div/div/router-view/div/div[1]/div[2]/div/ma-suggestion-control/div/div[1]/div[3]")
webElem$clickElement()
Sys.sleep(1)
webElem <- findElementCustom(using = "css selector", value = ".author-card")
remDr$screenshot(display = TRUE)
# Click author's name in the card and wait for profile
webElem <- findElementCustom(using = "xpath", value = "/html/body/div/div/div/router-view/ma-serp/div/div[3]/div/compose/div/ma-card/div/compose/div/div/div[1]/div/div/a/span")
stop <- tryCatch(
expr = {
webElem$clickElement()
FALSE
},
error = function(e) {
if (is_empty(webElem)) {
webElem <- findElementCustom(using = "css selector", value = ".ma-paper-results .au-target:nth-child(1) .au-target .au-target .au-target .ma-author-string-collection .au-target:nth-child(1) .link")
tryCatch(
expr = {
webElem$clickElement()
FALSE
},
error = function(e) {
message('No profile found; skipping...')
TRUE
}
)
}
},
finally = {
Sys.sleep(1)
}
)
if (stop) {
return(NULL)
}
webElem <- findElementCustom(using = "css selector", value = ".author .header , .name-section", attempts = 20)
remDr$screenshot(display = TRUE)
# Find affiliated university and extract text
webElem <- findElementCustom(using = "css selector", value = ".affiliation .au-target")
author_info$affiliation <- getElementTextCustom(webElem) %>%
as.character
# Find publications and extract text
webElem <- findElementCustom(using = "css selector", value = ".stats .au-target:nth-child(1) .ma-statistics-item .count")
author_info$publications <- getElementTextCustom(webElem) %>%
gsub(",", "", .) %>%
as.numeric
# Find citations and extract text
webElem <- findElementCustom(using = "css selector", value = ".stats .au-target:nth-child(2) .ma-statistics-item .count")
author_info$citations <- getElementTextCustom(webElem) %>%
gsub(",", "", .) %>%
as.numeric
# Find year of first publication and extract text
webElem <- findElementCustom(using = "css selector", value = "#filter-from .value")
author_info$year_start <- getElementTextCustom(webElem) %>%
as.numeric
# Find year of last publication and extract text
webElem <- findElementCustom(using = "css selector", value = "#filter-to .value")
author_info$year_end <- getElementTextCustom(webElem) %>%
as.numeric
# Find top topics and extract text
webElem <- findElementCustom(using = "css selector",
value = "ma-topics-filter .caption",
plural = TRUE)
author_info$top_topics <- getElementTextCustom(webElem, plural = TRUE)
# Find publication types and extract text
webElem <- findElementCustom(using = "css selector",
value = "ma-publication-type-filter .caption",
plural = TRUE)
author_info$publication_types <- getElementTextCustom(webElem, plural = TRUE)
# Find top authors and extract text
webElem <- findElementCustom(using = "css selector",
value = "ma-author-filter .caption",
plural = TRUE)
author_info$top_authors <- getElementTextCustom(webElem, plural = TRUE)
# Find top journals and extract text
webElem <- findElementCustom(using = "css selector",
value = "ma-journal-filter .caption",
plural = TRUE)
author_info$top_journals <- getElementTextCustom(webElem, plural = TRUE)
# Find top institutions and extract text
webElem <- findElementCustom(using = "css selector",
value = "ma-institution-filter .values",
plural = TRUE)
author_info$top_institutions <- getElementTextCustom(webElem, plural = TRUE)
# Find top conferences and extract text
webElem <- findElementCustom(using = "css selector",
value = "ma-conference-filter .caption",
plural = TRUE)
author_info$top_conferences <- getElementTextCustom(webElem, plural = TRUE)
# Return to homepage
remDr$navigate("https://academic.microsoft.com/home")
webElem <- findElementCustom(using = "css selector", value = ".top", attempts = 20)
Sys.sleep(10)
remDr$screenshot(display = TRUE)
return(author_info)
}
# linkedin-functions.R ----------------------------------------------------
# Part of our web crawler. Adds functions to the environment for scraping
# LinkedIn.
#
# Needs Docker Desktop installed and a keyring named “usc” configured with a
# username/password set up for a LinkedIn account. For more details on
# configuring a keyring, see the tutorial script
# “rselenium-chrome/keyring/r-keyring-tutorial.R”
#
# Outputs nothing.
#
# If there are questions/comments, please contact Matt (mdlee@usc.edu).
# -------------------------------------------------------------------------
# Loading required packages
suppressPackageStartupMessages(library("tidyverse"))
suppressPackageStartupMessages(library("lubridate"))
suppressPackageStartupMessages(library("RSelenium"))
suppressPackageStartupMessages(library("getProxy"))
suppressPackageStartupMessages(library("keyring"))
linkedinLogin <- function(email) {
if (!("usc" %in% keyring_list()$keyring)) {
stop('No keyring "usc" detected; please configure according to instructions in []')
}
if (nrow(key_list(service = "linkedin", keyring = "usc")) == 0) {
stop('No key "linkedin" (case sensitive) detected; please configure according to instructions in []')
}
remDr$navigate("https://www.linkedin.com/login")
webElem <- findElementCustom(using = "css selector", value = "#username")
webElem$clearElement()
webElem$sendKeysToElement(list(email))
Sys.sleep(1)
webElem <- findElementCustom(using = "css selector", value = "#password")
webElem$clearElement()
webElem$sendKeysToElement(list(key_get(service = "linkedin",
username = key_list(service = "linkedin", keyring = "usc")$username,
keyring = "usc")))
Sys.sleep(1)
webElem <- findElementCustom(using = "css selector", value = ".from__button--floating")
webElem$clickElement()
Sys.sleep(5)
if (grepl("add-phone", remDr$getCurrentUrl())) {
webElem <- findElementCustom(using = "css selector", value = ".secondary-action")
webElem$clickElement()
Sys.sleep(5)
}
#remDr$screenshot(display = TRUE)
}
linkedinGetProfileLinks <- function(query) {
webElem <- findElementCustom(using = "css selector", value = ".always-show-placeholder")
webElem$clearElement()
webElem$clickElement()
webElem$sendKeysToElement(list(query))
webElem$sendKeysToElement(list(key = "enter"))
Sys.sleep(5)
remDr$screenshot(display = TRUE)
webElem <- findElementCustom(using = "xpath", value = "//a[@href]", plural = TRUE)
all_links <- sapply(
webElem,
function(x) {
unlist(x$getElementAttribute("href"))
}
)
profile_links <- unique(all_links[which(grepl("/in/", all_links))])
}
linkedinGetProfileData <- function(profile_url) {
remDr$navigate(profile_url)
Sys.sleep(5)
remDr$screenshot(display = TRUE)
webElem <- findElementCustom(using = "id", value = "education-section")
text <- getElementTextCustom(webElem)
if (is.na(text)) {
highest_degree <- ""
degree_area <- ""
} else {
highest_degree <- str_extract(text, "(?<=Degree Name\\n).+?(?=(\\n)|.$)")
degree_area <- str_extract(text, "(?<=Field Of Study\\n).+?(?=(\\n)|.$)")
if (is.na(highest_degree) & is.na(degree_area)) {
highest_degree <- ""
degree_area <- ""
}
}
profile_info <- data.frame(
url = profile_url,
highest_degree = highest_degree,
degree_area = degree_area,
stringsAsFactors = FALSE
)
}
# test-microsoft-academic.R -----------------------------------------------
# Tests Microsoft Academic scraping. Navigates to the site, searches for
# Chris Mattmann and a few other authors, and views their information in
# a table.
#
# Needs Docker Desktop installed and both rselenium-base-functions.R and
# microsoft-academic-functions.R in the working directory.
# -------------------------------------------------------------------------
# Loading required packages
library("tidyverse")
library("lubridate")
library("RSelenium")
library("getProxy")
library("keyring")
# Loading setup functions
source("rselenium-base-functions.R")
source("microsoft-academic-functions.R")
tryCatch(
expr = {
startSession()
test <- mapply(
function(x, y) {
microsoftSearchAuthor(x, y)
},
x = c("Chris Mattmann",
"Ned Freed",
"Nathaniel Borenstein"),
y = c("A vision for data science",
" ",
" "),
SIMPLIFY = FALSE
) %>%
map(function(x) {x}) %>%
reduce(bind_rows)
},
warning = function(w) {
w
},
error = function(e) {
e
},
finally = {
endSession()
View(test)
}
)
# test-linkedin.R ---------------------------------------------------------
# Tests LinkedIn scraping. Logs in using a test account
# (testingtesting123yay@gmail.com), searches for Chris Mattmann, navigates
# to his profile, and pulls some information.
#
# Needs Docker Desktop installed and both rselenium-base-functions.R and
# linkedin-functions.R in the working directory.
#
# Also needs a keyring named "usc" set up with password to a "linkedin" service.
# For more information on configuring the keyring, see the tutorial script
# rselenium-chrome/keyring/r-keyring-tutorial.R
# -------------------------------------------------------------------------
# Loading required packages
library("tidyverse")
library("lubridate")
library("RSelenium")
library("getProxy")
library("keyring")
# Loading setup functions
source("rselenium-base-functions.R")
source("linkedin-functions.R")
tryCatch(
expr = {
message('Unlocking keyring named "usc"')
keyring_unlock(keyring = "usc")
startSession()
linkedinLogin("testingtesting123yay@gmail.com")
test_links <- linkedinGetProfileLinks("Chris Mattmann")
test <- linkedinGetProfileData(test_links[1])
},
warning = function(w) {
w
},
error = function(e) {
e
},
finally = {
endSession()
message('Found the following profile links; scraped the first one')
print(test_links)
View(test)
}
)
# test-linkedin.R ---------------------------------------------------------
# Tests LinkedIn scraping. Logs in using a test account
# (testingtesting123yay@gmail.com), searches for Chris Mattmann, navigates
# to his profile, and pulls some information.
#
# Needs Docker Desktop installed and both rselenium-base-functions.R and
# linkedin-functions.R in the working directory.
#
# Also needs a keyring named "usc" set up with password to a "linkedin" service.
# For more information on configuring the keyring, see the tutorial script
# rselenium-chrome/keyring/r-keyring-tutorial.R
# -------------------------------------------------------------------------
# Loading required packages
library("tidyverse")
library("lubridate")
library("RSelenium")
library("getProxy")
library("keyring")
# Loading setup functions
source("rselenium-base-functions.R")
source("linkedin-functions.R")
tryCatch(
expr = {
message('Unlocking keyring named "usc"')
keyring_unlock(keyring = "usc")
startSession()
linkedinLogin("testingtesting123yay@gmail.com")
test_links <- linkedinGetProfileLinks("Chris Mattmann")
test <- linkedinGetProfileData(test_links[1])
},
warning = function(w) {
w
},
error = function(e) {
e
},
finally = {
endSession()
message('Found the following profile links; scraped the first one')
print(test_links)
View(test)
}
)
# Unlock keyring "usc
keyring_unlock(keyring = "usc")
# test-linkedin.R ---------------------------------------------------------
# Tests LinkedIn scraping. Logs in using a test account
# (testingtesting123yay@gmail.com), searches for Chris Mattmann, navigates
# to his profile, and pulls some information.
#
# Needs Docker Desktop installed and both rselenium-base-functions.R and
# linkedin-functions.R in the working directory.
#
# Also needs a keyring named "usc" set up with password to a "linkedin" service.
# For more information on configuring the keyring, see the tutorial script
# rselenium-chrome/keyring/r-keyring-tutorial.R
# -------------------------------------------------------------------------
# Loading required packages
library("tidyverse")
library("lubridate")
library("RSelenium")
library("getProxy")
library("keyring")
# Loading setup functions
source("rselenium-base-functions.R")
source("linkedin-functions.R")
# Unlock keyring "usc
keyring_unlock(keyring = "usc")
tryCatch(
expr = {
startSession()
linkedinLogin("testingtesting123yay@gmail.com")
test_links <- linkedinGetProfileLinks("Chris Mattmann")
test <- linkedinGetProfileData(test_links[1])
},
warning = function(w) {
w
},
error = function(e) {
e
},
finally = {
endSession()
message('Found the following profile links; scraped the first one')
print(test_links)
View(test)
}
)
# test-linkedin.R ---------------------------------------------------------
# Tests LinkedIn scraping. Logs in using a test account
# (testingtesting123yay@gmail.com), searches for Chris Mattmann, navigates
# to his profile, and pulls some information.
#
# Needs Docker Desktop installed and both rselenium-base-functions.R and
# linkedin-functions.R in the working directory.
#
# Also needs a keyring named "usc" set up with password to a "linkedin" service.
# For more information on configuring the keyring, see the tutorial script
# rselenium-chrome/keyring/r-keyring-tutorial.R
# -------------------------------------------------------------------------
# Loading required packages
library("tidyverse")
library("lubridate")
library("RSelenium")
library("getProxy")
library("keyring")
# Loading setup functions
source("rselenium-base-functions.R")
source("linkedin-functions.R")
# Unlock keyring "usc
keyring_unlock(keyring = "usc")
tryCatch(
expr = {
startSession()
linkedinLogin("testingtesting123yay@gmail.com")
test_links <- linkedinGetProfileLinks("Chris A. Mattmann")
test <- linkedinGetProfileData(test_links[1])
},
warning = function(w) {
w
},
error = function(e) {
e
},
finally = {
endSession()
message('Found the following profile links; scraped the first one')
print(test_links)
View(test)
}
)
# test-linkedin.R ---------------------------------------------------------
# Tests LinkedIn scraping. Logs in using a test account
# (testingtesting123yay@gmail.com), searches for Chris Mattmann, navigates
# to his profile, and pulls some information.
#
# Needs Docker Desktop installed and both rselenium-base-functions.R and
# linkedin-functions.R in the working directory.
#
# Also needs a keyring named "usc" set up with password to a "linkedin" service.
# For more information on configuring the keyring, see the tutorial script
# rselenium-chrome/keyring/r-keyring-tutorial.R
# -------------------------------------------------------------------------
# Loading required packages
library("tidyverse")
library("lubridate")
library("RSelenium")
library("getProxy")
library("keyring")
# Loading setup functions
source("rselenium-base-functions.R")
source("linkedin-functions.R")
# Unlock keyring "usc"
keyring_unlock(keyring = "usc")
tryCatch(
expr = {
startSession()
linkedinLogin("testingtesting123yay@gmail.com")
test_links <- linkedinGetProfileLinks("Chris Mattmann JPL")
test <- linkedinGetProfileData(test_links[1])
},
warning = function(w) {
w
},
error = function(e) {
e
},
finally = {
endSession()
message('Found the following profile links; scraped the first one')
print(test_links)
View(test)
}
)
